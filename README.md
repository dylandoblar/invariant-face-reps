# Learning illumination-invariant representations for face verification

We investigate invariance properties of face representations under extreme illumination conditions.  In [model.py](./model.py), we implement the model proposed by Liao et al. in "Learning invariant representations and applications to face verification" which uses a templates-and-signatures approach to generate unique representations for query points unseen during training which are invariant under many "nice" transformations.  The model extracts face features from a set of template images, where several face identities have a small number of example images under different transformations from the transformation group of interest (in our case, we consider transformation groups corresponding to extreme illumination conditions; the original paper considers 2D affine transformations as well as yaw rotations).  Then, the model performs dimensionality reduction on this set of template features with PCA.  At test time, pairs of faces are presented and the model must determine whether faces in each pair have the same identity.  To do this, the model computes the features for each example (using the same method as used for the templates) and projects these features onto the principal components of the template features.  Then, the cosine similarity between each example and every template representation is computed, and mean pooling is used to construct representations for each query image in the pair, with a dimension for each template identity.  The cosine similarity of the query images is computed, and the model uses a simple thresholding decision rule to predict whether the query examples have the same identity.  Refer to the [original paper](https://papers.nips.cc/paper/2013/file/ad3019b856147c17e82a5bead782d2a8-Paper.pdf) for more details about the model.

In the feature extraction step, we implement a HOG feature extractor as described in the original paper.  Additionally, we allow for features generated by VGG-Face (proposed by Parkhi et al. in this [paper](https://www.robots.ox.ac.uk/~vgg/publications/2015/Parkhi15/)) to be used instead â€” we take the activations of the penultimate layer of a pre-trained model as features if this method is selected.
